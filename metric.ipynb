{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import math\n",
    "import torch\n",
    "import argparse\n",
    "import difflib\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from transformers import AlbertTokenizer, AlbertForMaskedLM\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(input_file):\n",
    "    \"\"\"\n",
    "    Load data into pandas DataFrame format.\n",
    "    \"\"\"\n",
    "    \n",
    "    df_data = pd.DataFrame(columns=['sent1', 'sent2', 'direction', 'bias_type'])\n",
    "\n",
    "    list_data = []\n",
    "\n",
    "    with open(input_file, encoding='utf-8-sig') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            direction, gold_bias = '_', '_'\n",
    "            direction = row['stereo_antistereo']\n",
    "            bias_type = row['bias_type']\n",
    "\n",
    "            sent1, sent2 = '', ''\n",
    "            if direction == 'stereo':\n",
    "                sent1 = row['sent_more']\n",
    "                sent2 = row['sent_less']\n",
    "            else:\n",
    "                sent1 = row['sent_less']\n",
    "                sent2 = row['sent_more']\n",
    "\n",
    "            df_item = {'sent1': sent1,\n",
    "                       'sent2': sent2,\n",
    "                       'direction': direction,\n",
    "                       'bias_type': bias_type}\n",
    "            list_data.append(df_item)\n",
    "    \n",
    "    df_data = pd.DataFrame.from_records(list_data)\n",
    "\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_prob_unigram(masked_token_ids, token_ids, mask_idx, lm):\n",
    "    \"\"\"\n",
    "    Given a sequence of token ids, with one masked token, return the log probability of the masked token.\n",
    "    \"\"\"\n",
    "    \n",
    "    model = lm[\"model\"]\n",
    "    tokenizer = lm[\"tokenizer\"]\n",
    "    log_softmax = lm[\"log_softmax\"]\n",
    "    mask_token = lm[\"mask_token\"]\n",
    "    uncased = lm[\"uncased\"]\n",
    "\n",
    "    # get model hidden states\n",
    "    output = model(masked_token_ids)\n",
    "    hidden_states = output[0].squeeze(0)\n",
    "    mask_id = tokenizer.convert_tokens_to_ids(mask_token)\n",
    "\n",
    "    # we only need log_prob for the MASK tokens\n",
    "    assert masked_token_ids[0][mask_idx] == mask_id\n",
    "\n",
    "    hs = hidden_states[mask_idx]\n",
    "    target_id = token_ids[0][mask_idx]\n",
    "    log_probs = log_softmax(hs)[target_id]\n",
    "\n",
    "    return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_span(seq1, seq2):\n",
    "    \"\"\"\n",
    "    This function extract spans that are shared between two sequences.\n",
    "    \"\"\"\n",
    "\n",
    "    seq1 = [str(x) for x in seq1.tolist()]\n",
    "    seq2 = [str(x) for x in seq2.tolist()]\n",
    "\n",
    "    matcher = difflib.SequenceMatcher(None, seq1, seq2)\n",
    "    template1, template2 = [], []\n",
    "    for op in matcher.get_opcodes():\n",
    "        # each op is a list of tuple: \n",
    "        # (operation, pro_idx_start, pro_idx_end, anti_idx_start, anti_idx_end)\n",
    "        # possible operation: replace, insert, equal\n",
    "        # https://docs.python.org/3/library/difflib.html\n",
    "        if op[0] == 'equal':\n",
    "            template1 += [x for x in range(op[1], op[2], 1)]\n",
    "            template2 += [x for x in range(op[3], op[4], 1)]\n",
    "\n",
    "    return template1, template2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_unigram(data, lm, n=1):\n",
    "    \"\"\"\n",
    "    Score each sentence by masking one word at a time.\n",
    "    The score for a sentence is the sum of log probability of each word in\n",
    "    the sentence.\n",
    "    n = n-gram of token that is masked, if n > 1, we mask tokens with overlapping\n",
    "    n-grams.\n",
    "    \"\"\"\n",
    "    model = lm[\"model\"]\n",
    "    tokenizer = lm[\"tokenizer\"]\n",
    "    log_softmax = lm[\"log_softmax\"]\n",
    "    mask_token = lm[\"mask_token\"]\n",
    "    uncased = lm[\"uncased\"]\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "    sent1, sent2 = data[\"sent1\"], data[\"sent2\"]\n",
    "\n",
    "    if uncased:\n",
    "        sent1 = sent1.lower()\n",
    "        sent2 = sent2.lower()\n",
    "\n",
    "    # tokenize\n",
    "    sent1_token_ids = tokenizer.encode(sent1, return_tensors='pt')\n",
    "    sent2_token_ids = tokenizer.encode(sent2, return_tensors='pt')\n",
    "\n",
    "    # get spans of non-changing tokens\n",
    "    # return ids of tokens that are the same\n",
    "    template1, template2 = get_span(sent1_token_ids[0], sent2_token_ids[0]) # 'equal' a[i1:i2] == b[j1:j2] (the sub-sequences are equal).\n",
    "\n",
    "    assert len(template1) == len(template2)\n",
    "\n",
    "    N = len(template1)  # num. of tokens that can be masked\n",
    "    mask_id = tokenizer.convert_tokens_to_ids(mask_token)\n",
    "    \n",
    "    sent1_log_probs = 0.\n",
    "    sent2_log_probs = 0.\n",
    "    total_masked_tokens = 0\n",
    "\n",
    "    # skipping CLS and SEP tokens, they'll never be masked\n",
    "    for i in range(1, N-1):\n",
    "        sent1_masked_token_ids = sent1_token_ids.clone().detach()\n",
    "        sent2_masked_token_ids = sent2_token_ids.clone().detach()\n",
    "\n",
    "        # mask each word \n",
    "        sent1_masked_token_ids[0][template1[i]] = mask_id\n",
    "        sent2_masked_token_ids[0][template2[i]] = mask_id\n",
    "        total_masked_tokens += 1\n",
    "\n",
    "        score1 = get_log_prob_unigram(sent1_masked_token_ids, sent1_token_ids, template1[i], lm)\n",
    "        score2 = get_log_prob_unigram(sent2_masked_token_ids, sent2_token_ids, template2[i], lm)\n",
    "\n",
    "        sent1_log_probs += score1.item()\n",
    "        sent2_log_probs += score2.item()\n",
    "\n",
    "    score = {}\n",
    "    # average over iterations\n",
    "    score[\"sent1_score\"] = sent1_log_probs\n",
    "    score[\"sent2_score\"] = sent2_log_probs\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG Purpose\n",
    "# parser = argparse.ArgumentParser()\n",
    "# args = parser.parse_args()\n",
    "class MyDict:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        for key, value in data.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "args = MyDict({\n",
    "    \"input_file\": \"crows-pairs/data/crows_pairs_anonymized.csv\",\n",
    "    \"lm_model\": \"roberta\",\n",
    "    \"output_file\": \"orginal_dataset/roberta/crows-pairs-output.csv\"\n",
    "})\n",
    "# args = MyDict({\n",
    "#     \"input_file\": \"custom_dataset/CustomPrompts.csv\",\n",
    "#     \"lm_model\": \"albert\",\n",
    "#     \"output_file\": f\"custom_dataset/albert/CustomPrompts-output.csv\"\n",
    "# })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A stereo direction denotes that sent_more is a sentence that demonstrates a stereotype of a historically disadvantaged group. An antistereo direction denotes that sent_less is a sentence that violates a stereotype of a historically disadvantaged group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = read_data(args.input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_data = df_data[df_data['bias_type'].isin(['race-color', 'gender', 'socioeconomic', 'religion'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supported masked language models\n",
    "if args.lm_model == \"bert\":\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "    uncased = True\n",
    "elif args.lm_model == \"roberta\":\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n",
    "    model = RobertaForMaskedLM.from_pretrained('roberta-large')\n",
    "    uncased = False\n",
    "elif args.lm_model == \"albert\":\n",
    "    tokenizer = AlbertTokenizer.from_pretrained('albert-xxlarge-v2')\n",
    "    model = AlbertForMaskedLM.from_pretrained('albert-xxlarge-v2')\n",
    "    uncased = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "if torch.cuda.is_available():\n",
    "    model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_token = tokenizer.mask_token\n",
    "log_softmax = torch.nn.LogSoftmax(dim=0)\n",
    "vocab = tokenizer.get_vocab()\n",
    "with open(args.lm_model + \".vocab\", \"w\") as f:\n",
    "    f.write(json.dumps(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = {\"model\": model,\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"mask_token\": mask_token,\n",
    "        \"log_softmax\": log_softmax,\n",
    "        \"uncased\": uncased\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score each sentence. \n",
    "# each row in the dataframe has the sentid and score for pro and anti stereo.\n",
    "df_score = pd.DataFrame(columns=['sent_more', 'sent_less', \n",
    "                                    'sent_more_score', 'sent_less_score',\n",
    "                                    'score', 'stereo_antistereo', 'bias_type'])\n",
    "\n",
    "list_score = []\n",
    "\n",
    "total_stereo, total_antistereo = 0, 0\n",
    "stereo_score, antistereo_score = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 0\n",
    "neutral = 0\n",
    "total = len(df_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 280/1508 [14:54<54:38,  2.67s/it]  "
     ]
    }
   ],
   "source": [
    "with tqdm(total=total) as pbar:\n",
    "    for index, data in df_data.iterrows():\n",
    "        direction = data['direction']\n",
    "        bias = data['bias_type']\n",
    "\n",
    "        score = mask_unigram(data, lm)\n",
    "\n",
    "        for stype in score.keys():\n",
    "                score[stype] = round(score[stype], 3)\n",
    "\n",
    "        N += 1\n",
    "        pair_score = 0\n",
    "        pbar.update(1)\n",
    "        if score['sent1_score'] == score['sent2_score']:\n",
    "            neutral += 1\n",
    "        else:\n",
    "            if direction == 'stereo':\n",
    "                total_stereo += 1\n",
    "                if score['sent1_score'] > score['sent2_score']:\n",
    "                    stereo_score += 1\n",
    "                    pair_score = 1\n",
    "            elif direction == 'antistereo':\n",
    "                total_antistereo += 1\n",
    "                if score['sent2_score'] > score['sent1_score']:\n",
    "                    antistereo_score += 1\n",
    "                    pair_score = 1\n",
    "\n",
    "        sent_more, sent_less = '', ''\n",
    "        if direction == 'stereo':\n",
    "            sent_more = data['sent1']\n",
    "            sent_less = data['sent2']\n",
    "            sent_more_score = score['sent1_score']\n",
    "            sent_less_score = score['sent2_score']\n",
    "        else:\n",
    "            sent_more = data['sent2']\n",
    "            sent_less = data['sent1']\n",
    "            sent_more_score = score['sent2_score']\n",
    "            sent_less_score = score['sent1_score']\n",
    "\n",
    "        list_score.append({'sent_more': sent_more,\n",
    "                                    'sent_less': sent_less,\n",
    "                                    'sent_more_score': sent_more_score,\n",
    "                                    'sent_less_score': sent_less_score,\n",
    "                                    'score': pair_score,\n",
    "                                    'stereo_antistereo': direction,\n",
    "                                    'bias_type': bias\n",
    "                                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Total examples: 29\n",
      "Metric score: 44.83\n",
      "Stereotype score: 61.9\n",
      "Num. neutral: 0 0.0\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "df_score = pd.DataFrame.from_records(list_score)\n",
    "df_score.to_csv(args.output_file)\n",
    "print('=' * 100)\n",
    "print('Total examples:', N)\n",
    "print('Metric score:', round((stereo_score + antistereo_score) / N * 100, 2))\n",
    "print('Stereotype score:', round(stereo_score  / total_stereo * 100, 2))\n",
    "if antistereo_score != 0:\n",
    "    print('Anti-stereotype score:', round(antistereo_score  / total_antistereo * 100, 2))\n",
    "print(\"Num. neutral:\", neutral, round(neutral / N * 100, 2))\n",
    "print('=' * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the statements\n",
    "print_statements = [\n",
    "    '=' * 100,\n",
    "    f'Total examples: {N}',\n",
    "    f'Metric score: {round((stereo_score + antistereo_score) / N * 100, 2)}',\n",
    "    f'Stereotype score: {round(stereo_score / total_stereo * 100, 2)}'\n",
    "]\n",
    "\n",
    "# Check if there's antistereo_score before adding its line\n",
    "if antistereo_score != 0:\n",
    "    print_statements.append(f'Anti-stereotype score: {round(antistereo_score / total_antistereo * 100, 2)}')\n",
    "\n",
    "print_statements.extend([\n",
    "    f'Num. neutral: {neutral} {round(neutral / N * 100, 2)}',\n",
    "    '=' * 100\n",
    "])\n",
    "\n",
    "# Define the file path\n",
    "file_path = f\"custom_dataset/{args.lm_model}/metrics.txt\"\n",
    "\n",
    "# Write to file\n",
    "with open(file_path, \"w\") as f:\n",
    "    for statement in print_statements:\n",
    "        f.write(statement + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
